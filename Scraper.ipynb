{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "#selenium imports\n",
    "#install geckodriver first and make sure you have selenium installed and firefox is a version > 53.0\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Restaurant:\n",
    "    \"\"\"\n",
    "    Represents a restaurant with all the menu.\n",
    "    Used for scraping before covnerting everything to csv.\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "    def scrape(self):\n",
    "        self.html = urllib.request.urlopen(url).read()\n",
    "        self.soup = BeautifulSoup(self.html, \"lxml\")\n",
    "        self.scrape_menu()\n",
    "        self.scrape_info()\n",
    "        self.clean_scrape()\n",
    "        with open(\"restaurants_pages/{}.html\".format(self.restaurant_name), \"w\") as htmlfile:\n",
    "            htmlfile.write(str(self.html))\n",
    "        # Cleanup to avoid using too much memo \n",
    "        del self.html\n",
    "        del self.soup\n",
    "        \n",
    "        \n",
    "    def scrape_menu(self):\n",
    "        self.menu_items, self.menu_prices = map(self.scrape_all_class, \n",
    "                                                [\"cardCategory-itemTitle\",\"cardCategory-itemPrice\"])\n",
    "\n",
    "    def scrape_info(self):\n",
    "        self.avg_price = self.scrape_all_class(\"pull-left restaurantSummary-price\", True)\n",
    "        self.tags = self.scrape_all_class(\"restaurantTag\")\n",
    "        self.location = self.scrape_all_class(\"restaurantSummary-address\", True)\n",
    "        self.restaurant_name = self.scrape_all_class(\"restaurantSummary-name\", True)\n",
    "        self.glob_rating = self.scrape_all_class(\"rating-ratingValue\", True)\n",
    "        \n",
    "    def scrape_all_class(self, name, first=False):\n",
    "        if first:\n",
    "            return self.soup.find(class_=name)\n",
    "        else:\n",
    "            return list(self.soup.find_all(class_=name))\n",
    "        \n",
    "    def clean_scrape(self):\n",
    "        def get_clean_text(string):\n",
    "            return string.text.strip()\n",
    "        \n",
    "        def prices_as_numbers(price):\n",
    "            return float(replace_dict(price, {\"€\": \"\", \"\\xa0\": \"\", \",\": \".\", \" \": \"\"}))\n",
    "        \n",
    "        def replace_dict(text, changes):\n",
    "            for old, new in changes.items():\n",
    "                text = text.replace(old, new)\n",
    "            return text\n",
    "        \n",
    "        self.menu_items = map(get_clean_text, self.menu_items)\n",
    "        self.tags = map(get_clean_text, self.tags)\n",
    "        self.location = get_clean_text(self.location)\n",
    "        self.restaurant_name = get_clean_text(self.restaurant_name)\n",
    "        self.menu_prices = map(get_clean_text, self.menu_prices)\n",
    "        self.menu_prices = map(prices_as_numbers, self.menu_prices)\n",
    "        self.avg_price = prices_as_numbers(get_clean_text(self.avg_price)[-8:])\n",
    "        self.glob_rating = prices_as_numbers(get_clean_text(self.glob_rating))\n",
    "    \n",
    "    def iter_menu(self):\n",
    "        for item, price in zip(self.menu_items, self.menu_prices):\n",
    "            yield (self.restaurant_name, item, price, self.url, self.glob_rating, self.avg_price,\n",
    "                   self.location, list(self.tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Le 23 Clauzel - Julie Rivière -'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.lafourchette.com/restaurant/le-23-clauzel-julie-riviere/6999\"\n",
    "rest = Restaurant(url)\n",
    "rest.scrape()\n",
    "rest.restaurant_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the list of restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#introducting the functions for wait times\n",
    "def wait(dr, x, t,i):\n",
    "    element = WebDriverWait(dr, t).until(EC.text_to_be_present_in_element((By.XPATH, x),i))\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## check code to make sure the driver works, don't run it each time##########\n",
    "#dr = webdriver.PhantomJS(service_args=['--ignore-ssl-errors=true'])\n",
    "dr = webdriver.Firefox()\n",
    "dr.get(\"https://www.lafourchette.com/restaurant+paris#sort=QUALITY_DESC&page=105\")\n",
    "wait(dr,\"//li[@class='active']\",10,\"105\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### check code to see if it works with beautifulsoup::::: NOT parsing all the strings but still does a good work###########\n",
    "soup = BeautifulSoup(dr.page_source, 'lxml')\n",
    "base_url = \"https://www.lafourchette.com\"\n",
    "pattern = re.compile(\"/restaurant/[a-z-]+/[0-9]+$\")\n",
    "restaurants = []\n",
    "restaurants.extend(base_url + rest[\"href\"] for rest in soup.find_all(href=pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"temp_page_source.html\",\"w\") as f:\n",
    "    f.write(dr.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(restaurants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### FIXED the find_restaurants #########\n",
    "\n",
    "def find_restaurants():\n",
    "    \n",
    "    base_url_search = \"https://www.lafourchette.com\"\n",
    "    base_url = \"https://www.lafourchette.com/\"\n",
    "    search_url = base_url + \"restaurant+paris#sort=QUALITY_DESC&page={}\"\n",
    "    pattern = re.compile(\"restaurant/[a-z-]+/[0-9]+$\")\n",
    "    restaurants = []\n",
    "    for i in range(1, 255):\n",
    "        dr = webdriver.Firefox()\n",
    "        if i % 10 == 0:\n",
    "            print(\"Handled {} pages, have {} restaurant urls\".format(i, len(restaurants)))\n",
    "        search_page = search_url.format(i)\n",
    "        dr.get(search_page)\n",
    "        wait(dr,\"//li[@class='active']\",10,str(i))\n",
    "        soup = BeautifulSoup(dr.page_source, \"lxml\")\n",
    "        temp = []\n",
    "        temp.extend(base_url_search + rest[\"href\"] for rest in soup.find_all(href=pattern))\n",
    "        temp = list(set(temp))\n",
    "        #restaurants.extend(base_url + rest[\"href\"] for rest in soup.find_all(href=pattern))\n",
    "        restaurants.extend(temp)\n",
    "        dr.quit()\n",
    "    \n",
    "    return restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handled 10 pages, have 218 restaurant urls\n",
      "Handled 20 pages, have 458 restaurant urls\n",
      "Handled 30 pages, have 695 restaurant urls\n",
      "Handled 40 pages, have 930 restaurant urls\n",
      "Handled 50 pages, have 1163 restaurant urls\n",
      "Handled 60 pages, have 1401 restaurant urls\n",
      "Handled 70 pages, have 1640 restaurant urls\n",
      "Handled 80 pages, have 1877 restaurant urls\n",
      "Handled 90 pages, have 2119 restaurant urls\n",
      "Handled 100 pages, have 2361 restaurant urls\n",
      "Handled 110 pages, have 2602 restaurant urls\n",
      "Handled 120 pages, have 2843 restaurant urls\n",
      "Handled 130 pages, have 3080 restaurant urls\n",
      "Handled 140 pages, have 3322 restaurant urls\n",
      "Handled 150 pages, have 3563 restaurant urls\n",
      "Handled 160 pages, have 3801 restaurant urls\n",
      "Handled 170 pages, have 4040 restaurant urls\n",
      "Handled 180 pages, have 4280 restaurant urls\n",
      "Handled 190 pages, have 4519 restaurant urls\n",
      "Handled 200 pages, have 4764 restaurant urls\n",
      "Handled 210 pages, have 5007 restaurant urls\n",
      "Handled 220 pages, have 5135 restaurant urls\n",
      "Handled 230 pages, have 5135 restaurant urls\n",
      "Handled 240 pages, have 5135 restaurant urls\n",
      "Handled 250 pages, have 5135 restaurant urls\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5135"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Will take a lot of time to complete ~1-2 hrs depending on the internet speed. Couldn't get phantomJS to work \n",
    "#### or would  have been faster ####\n",
    "\n",
    "restaurants = find_restaurants()\n",
    "\n",
    "len(restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving in case we need to reload..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(restaurants).to_csv(\"rest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv(\"rest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 50 restaurants\n",
      "Saving menu...\n",
      "Parsed 100 restaurants\n",
      "Saving menu...\n",
      "Parsed 150 restaurants\n",
      "Saving menu...\n",
      "Parsed 200 restaurants\n",
      "Saving menu...\n",
      "Parsed 250 restaurants\n",
      "Saving menu...\n",
      "Parsed 300 restaurants\n",
      "Saving menu...\n",
      "Parsed 350 restaurants\n",
      "Saving menu...\n",
      "Parsed 400 restaurants\n",
      "Saving menu...\n",
      "Parsed 450 restaurants\n",
      "Saving menu...\n",
      "Parsed 500 restaurants\n",
      "Saving menu...\n",
      "Parsed 550 restaurants\n",
      "Saving menu...\n",
      "Parsed 600 restaurants\n",
      "Saving menu...\n",
      "Parsed 650 restaurants\n",
      "Saving menu...\n",
      "Parsed 700 restaurants\n",
      "Saving menu...\n",
      "Parsed 750 restaurants\n",
      "Saving menu...\n",
      "Parsed 800 restaurants\n",
      "Saving menu...\n",
      "Parsed 850 restaurants\n",
      "Saving menu...\n",
      "Parsed 900 restaurants\n",
      "Saving menu...\n",
      "Parsed 950 restaurants\n",
      "Saving menu...\n",
      "Parsed 1000 restaurants\n",
      "Saving menu...\n",
      "Parsed 1050 restaurants\n",
      "Saving menu...\n",
      "Parsed 1100 restaurants\n",
      "Saving menu...\n",
      "Parsed 1150 restaurants\n",
      "Saving menu...\n",
      "Parsed 1200 restaurants\n",
      "Saving menu...\n",
      "Parsed 1250 restaurants\n",
      "Saving menu...\n",
      "Parsed 1300 restaurants\n",
      "Saving menu...\n",
      "Parsed 1350 restaurants\n",
      "Saving menu...\n",
      "Parsed 1400 restaurants\n",
      "Saving menu...\n",
      "Parsed 1450 restaurants\n",
      "Saving menu...\n",
      "Parsed 1500 restaurants\n",
      "Saving menu...\n",
      "Parsed 1550 restaurants\n",
      "Saving menu...\n",
      "Parsed 1600 restaurants\n",
      "Saving menu...\n",
      "Parsed 1650 restaurants\n",
      "Saving menu...\n",
      "Parsed 1700 restaurants\n",
      "Saving menu...\n",
      "Parsed 1750 restaurants\n",
      "Saving menu...\n",
      "Parsed 1800 restaurants\n",
      "Saving menu...\n",
      "Parsed 1850 restaurants\n",
      "Saving menu...\n",
      "Parsed 1900 restaurants\n",
      "Saving menu...\n",
      "Parsed 1950 restaurants\n",
      "Saving menu...\n",
      "Parsed 2000 restaurants\n",
      "Saving menu...\n",
      "Parsed 2050 restaurants\n",
      "Saving menu...\n",
      "Parsed 2100 restaurants\n",
      "Saving menu...\n",
      "Parsed 2150 restaurants\n",
      "Saving menu...\n",
      "Parsed 2200 restaurants\n",
      "Saving menu...\n",
      "Parsed 2250 restaurants\n",
      "Saving menu...\n",
      "Parsed 2300 restaurants\n",
      "Saving menu...\n",
      "Parsed 2350 restaurants\n",
      "Saving menu...\n",
      "Parsed 2400 restaurants\n",
      "Saving menu...\n",
      "Parsed 2450 restaurants\n",
      "Saving menu...\n",
      "Parsed 2500 restaurants\n",
      "Saving menu...\n",
      "Parsed 2550 restaurants\n",
      "Saving menu...\n",
      "Parsed 2600 restaurants\n",
      "Saving menu...\n",
      "Parsed 2650 restaurants\n",
      "Saving menu...\n",
      "Parsed 2700 restaurants\n",
      "Saving menu...\n",
      "Parsed 2750 restaurants\n",
      "Saving menu...\n",
      "Parsed 2800 restaurants\n",
      "Saving menu...\n",
      "Parsed 2850 restaurants\n",
      "Saving menu...\n",
      "Parsed 2900 restaurants\n",
      "Saving menu...\n",
      "Parsed 2950 restaurants\n",
      "Saving menu...\n",
      "Parsed 3000 restaurants\n",
      "Saving menu...\n",
      "Parsed 3050 restaurants\n",
      "Saving menu...\n",
      "Parsed 3100 restaurants\n",
      "Saving menu...\n",
      "Parsed 3150 restaurants\n",
      "Saving menu...\n",
      "Parsed 3200 restaurants\n",
      "Saving menu...\n",
      "Parsed 3250 restaurants\n",
      "Saving menu...\n",
      "Parsed 3300 restaurants\n",
      "Saving menu...\n",
      "Parsed 3350 restaurants\n",
      "Saving menu...\n",
      "Parsed 3400 restaurants\n",
      "Saving menu...\n",
      "Parsed 3450 restaurants\n",
      "Saving menu...\n",
      "Parsed 3500 restaurants\n",
      "Saving menu...\n",
      "Parsed 3550 restaurants\n",
      "Saving menu...\n",
      "Parsed 3600 restaurants\n",
      "Saving menu...\n",
      "Parsed 3650 restaurants\n",
      "Saving menu...\n",
      "Parsed 3700 restaurants\n",
      "Saving menu...\n",
      "Parsed 3750 restaurants\n",
      "Saving menu...\n",
      "Parsed 3800 restaurants\n",
      "Saving menu...\n",
      "Parsed 3850 restaurants\n",
      "Saving menu...\n",
      "Parsed 3900 restaurants\n",
      "Saving menu...\n",
      "Parsed 3950 restaurants\n",
      "Saving menu...\n",
      "Parsed 4000 restaurants\n",
      "Saving menu...\n",
      "Parsed 4050 restaurants\n",
      "Saving menu...\n",
      "Parsed 4100 restaurants\n",
      "Saving menu...\n",
      "Parsed 4150 restaurants\n",
      "Saving menu...\n",
      "Parsed 4200 restaurants\n",
      "Saving menu...\n",
      "Parsed 4250 restaurants\n",
      "Saving menu...\n",
      "Parsed 4300 restaurants\n",
      "Saving menu...\n",
      "Parsed 4350 restaurants\n",
      "Saving menu...\n",
      "Parsed 4400 restaurants\n",
      "Saving menu...\n",
      "Parsed 4450 restaurants\n",
      "Saving menu...\n",
      "Parsed 4500 restaurants\n",
      "Saving menu...\n",
      "Parsed 4550 restaurants\n",
      "Saving menu...\n",
      "Parsed 4600 restaurants\n",
      "Saving menu...\n",
      "Parsed 4650 restaurants\n",
      "Saving menu...\n",
      "Parsed 4700 restaurants\n",
      "Saving menu...\n",
      "Parsed 4750 restaurants\n",
      "Saving menu...\n",
      "Parsed 4800 restaurants\n",
      "Saving menu...\n"
     ]
    }
   ],
   "source": [
    "### We want to make sure we save periodically given all the issues\n",
    "import csv\n",
    "\n",
    "fails = 0\n",
    "obj_rests = []\n",
    "failed = []\n",
    "\n",
    "\n",
    "with open(\"all_items.csv\", \"w\") as csvfile:\n",
    "    # Use tab because \",\" or \";\" might be used in description\n",
    "    writer = csv.writer(csvfile, delimiter='\\t')\n",
    "    writer.writerow([\"rest_name\", \"item\", \"item_price\", \"rest_url\", \"rest_global_rating\",\n",
    "                     \"rest_avg_price\", \"rest_location\", \"rest_tags\"])\n",
    "\n",
    "    \n",
    "for i, url in enumerate(restaurants.restaurants):\n",
    "    if not (i+1) % 50:\n",
    "        print(\"Parsed {} restaurants\".format(i+1))\n",
    "        print(\"Saving menu...\")\n",
    "        \n",
    "        with open(\"all_items.csv\", \"a+\") as csvfile:\n",
    "        # Use tab because \",\" or \";\" might be used in description\n",
    "            writer = csv.writer(csvfile, delimiter='\\t')\n",
    "            for restaurant_menu in obj_rests[i-50:]:\n",
    "                try:\n",
    "                    writer.writerows(restaurant_menu.iter_menu())\n",
    "                except:\n",
    "                    fails += 1\n",
    "                    \n",
    "    try:\n",
    "        rest = Restaurant(url)\n",
    "        rest.scrape()\n",
    "        obj_rests.append(rest)\n",
    "    except:\n",
    "        failed.append(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while pickling an object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1fea77b052fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"restaurants.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_rests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while pickling an object"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "output = open(\"restaurants.pkl\", \"wb\")\n",
    "pickle.dump(obj_rests, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_2 = pd.read_csv(\"all_items.csv\", sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
