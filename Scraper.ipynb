{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#selenium imports\n",
    "#install geckodriver first and make sure you have selenium installed and firefox is a version > 53.0\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Restaurant:\n",
    "    \"\"\"\n",
    "    Represents a restaurant with all the menu.\n",
    "    Used for scraping before covnerting everything to csv.\n",
    "    \"\"\"\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "    def scrape(self):\n",
    "        self.soup = BeautifulSoup(urllib.request.urlopen(url).read(), \"lxml\")\n",
    "        self.scrape_menu()\n",
    "        self.scrape_info()\n",
    "        self.clean_scraped()\n",
    "        \n",
    "        \n",
    "    def scrape_menu(self):\n",
    "        self.menu_items, self.menu_prices = map(self.scrape_all_class, \n",
    "                                                [\"cardCategory-itemTitle\",\"cardCategory-itemPrice\"])\n",
    "\n",
    "    def scrape_info(self):\n",
    "        self.avg_price = self.scrape_all_class(\"pull-left restaurantSummary-price\", True)\n",
    "        self.tags = self.scrape_all_class(\"restaurantTag\")\n",
    "        self.location = self.scrape_all_class(\"restaurantSummary-address\", True)\n",
    "        self.restaurant_name = self.scrape_all_class(\"restaurantSummary-name\", True)\n",
    "        self.glob_rating = self.scrape_all_class(\"rating-ratingValue\", True)\n",
    "        \n",
    "    def scrape_all_class(self, name, first=False):\n",
    "        if first:\n",
    "            return self.soup.find(class_=name)\n",
    "        else:\n",
    "            return self.soup.find_all(class_=name)\n",
    "        \n",
    "    def clean_scraped(self):\n",
    "        def get_clean_text(string):\n",
    "            return string.text.strip()\n",
    "        \n",
    "        def prices_as_numbers(price):\n",
    "            return float(replace_dict(price, {\"€\": \"\", \"\\xa0\": \"\", \",\": \".\", \" \": \"\"}))\n",
    "        \n",
    "        def replace_dict(text, changes):\n",
    "            for old, new in changes.items():\n",
    "                text = text.replace(old, new)\n",
    "            return text\n",
    "        \n",
    "        self.menu_items = map(get_clean_text, self.menu_items,)\n",
    "        self.tags = map(get_clean_text, self.tags,)\n",
    "        self.location = get_clean_text(self.location)\n",
    "        self.restaurant_name = get_clean_text(self.restaurant_name)\n",
    "        self.menu_prices = map(get_clean_text, self.menu_prices)\n",
    "        self.menu_prices = map(prices_as_numbers, self.menu_prices)\n",
    "        self.avg_price = prices_as_numbers(get_clean_text(self.avg_price)[-8:])\n",
    "        self.glob_rating = prices_as_numbers(get_clean_text(self.glob_rating))\n",
    "    \n",
    "    def iter_menu(self):\n",
    "        for item, price in zip(self.menu_items, self.menu_prices):\n",
    "            yield (self.restaurant_name, item, price, self.url, self.glob_rating, self.avg_price,\n",
    "                   self.location, list(self.tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Le 23 Clauzel - Julie Rivière -'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.lafourchette.com/restaurant/le-23-clauzel-julie-riviere/6999\"\n",
    "rest = Restaurant(url)\n",
    "rest.scrape()\n",
    "rest.restaurant_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the list of restaurants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#introducting the functions for wait times\n",
    "def wait(dr, x, t,i):\n",
    "    element = WebDriverWait(dr, t).until(EC.text_to_be_present_in_element((By.XPATH, x),i))\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## check code to make sure the driver works, don't run it each time##########\n",
    "#dr = webdriver.PhantomJS(service_args=['--ignore-ssl-errors=true'])\n",
    "dr = webdriver.Firefox()\n",
    "dr.get(\"https://www.lafourchette.com/restaurant+paris#sort=QUALITY_DESC&page=105\")\n",
    "wait(dr,\"//li[@class='active']\",10,\"105\")\n",
    "\n",
    "#restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####### check code to see if it works with beautifulsoup::::: NOT parsing all the strings but still does a good work###########\n",
    "soup = BeautifulSoup(dr.page_source,'html.parser')\n",
    "base_url = \"https://www.lafourchette.com\"\n",
    "pattern = re.compile(\"/restaurant/[a-z-]+/[0-9]+$\")\n",
    "restaurants = []\n",
    "restaurants.extend(base_url + rest[\"href\"] for rest in soup.find_all(href=pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"temp_page_source.html\",\"w\") as f:\n",
    "    f.write(dr.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(restaurants)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Buggy: see fb chat \n",
    "# Need to implement: \n",
    "# https://stackoverflow.com/questions/37953182/how-do-i-wait-through-a-wait-page-and-then-download-a-pdf-using-python\n",
    "\n",
    "###### FIXED the find_restaurants #########3\n",
    "\n",
    "\n",
    "def find_restaurants():\n",
    "    \n",
    "    base_url_search = \"https://www.lafourchette.com\"\n",
    "    base_url = \"https://www.lafourchette.com/\"\n",
    "    search_url = base_url + \"restaurant+paris#sort=QUALITY_DESC&page={}\"\n",
    "    pattern = re.compile(\"restaurant/[a-z-]+/[0-9]+$\")\n",
    "    restaurants = []\n",
    "    for i in range(1, 255):\n",
    "        dr = webdriver.Firefox()\n",
    "        if i % 10 == 0:\n",
    "            print(\"Handled {} pages, have {} restaurant urls\".format(i, len(restaurants)))\n",
    "        search_page = search_url.format(i)\n",
    "        dr.get(search_page)\n",
    "        wait(dr,\"//li[@class='active']\",10,str(i))\n",
    "        soup = BeautifulSoup(dr.page_source, \"lxml\")\n",
    "        temp = []\n",
    "        temp.extend(base_url_search + rest[\"href\"] for rest in soup.find_all(href=pattern))\n",
    "        temp = list(set(temp))\n",
    "        #restaurants.extend(base_url + rest[\"href\"] for rest in soup.find_all(href=pattern))\n",
    "        restaurants.extend(temp)\n",
    "        dr.quit()\n",
    "    \n",
    "    return restaurants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handled 10 pages, have 218 restaurant urls\n",
      "Handled 20 pages, have 456 restaurant urls\n",
      "Handled 30 pages, have 694 restaurant urls\n",
      "Handled 40 pages, have 930 restaurant urls\n",
      "Handled 50 pages, have 1161 restaurant urls\n",
      "Handled 60 pages, have 1399 restaurant urls\n",
      "Handled 70 pages, have 1636 restaurant urls\n",
      "Handled 80 pages, have 1875 restaurant urls\n",
      "Handled 90 pages, have 2117 restaurant urls\n",
      "Handled 100 pages, have 2358 restaurant urls\n",
      "Handled 110 pages, have 2600 restaurant urls\n",
      "Handled 120 pages, have 2841 restaurant urls\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: Reached error page: about:neterror?e=dnsNotFound&u=https%3A//www.lafourchette.com/restaurant+paris%23sort%3DQUALITY_DESC%26page%3D122&c=UTF-8&f=regular&d=Firefox%20can%E2%80%99t%20find%20the%20server%20at%20www.lafourchette.com.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-44c5f978ff10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrestaurants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_restaurants\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestaurants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6d483ec03846>\u001b[0m in \u001b[0;36mfind_restaurants\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Handled {} pages, have {} restaurant urls\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestaurants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msearch_page\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"//li[@class='active']\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mLoads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mweb\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mbrowser\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \"\"\"\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    310\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'alert'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Reached error page: about:neterror?e=dnsNotFound&u=https%3A//www.lafourchette.com/restaurant+paris%23sort%3DQUALITY_DESC%26page%3D122&c=UTF-8&f=regular&d=Firefox%20can%E2%80%99t%20find%20the%20server%20at%20www.lafourchette.com.\n"
     ]
    }
   ],
   "source": [
    "### Will take a lot of time to complete ~1-2 hrs depending on the internet speed. Couldn't get phantomJS to work \n",
    "#### or would  have been faster ####\n",
    "\n",
    "restaurants = find_restaurants()\n",
    "\n",
    "len(restaurants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5135"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(restaurants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'restaurants' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c6cf04b3a853>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrestaurants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'restaurants' is not defined"
     ]
    }
   ],
   "source": [
    "restaurants[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'restaurants' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66f26c070d21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mobj_rests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrestaurants\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRestaurant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mrest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'restaurants' is not defined"
     ]
    }
   ],
   "source": [
    "''' object rest contains all the restaurants which were able to be parsed. Could be made more robust to introduce\n",
    "    the try and catch in the class restaurant to make sure the ones which have the important urls are kept\n",
    "'''\n",
    "obj_rests = []\n",
    "for url in restaurants:\n",
    "    try:\n",
    "        rest = Restaurant(url)\n",
    "        rest.scrape()\n",
    "        obj_rests.append(rest)\n",
    "        print('Succesfully parsed' , rest.restaurant_name)\n",
    "    except:\n",
    "        print('Not able to parse' , rest.restaurant_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
