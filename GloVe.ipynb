{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe : getting relevant word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import random\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('all_items_improved.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[data.item_price > 0]\n",
    "data = data[data.item_price < 200]\n",
    "data['rest_location'] = data['rest_location'].apply(lambda x: x.split(\"\\n\")[1].strip())\n",
    "data['rest_location'] = data['rest_location'].replace('', '0').replace('Bercy Village', '0').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For speed, we're only using the 250 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_window = 4\n",
    "top_k = 250\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.lower().split()\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for example in data.item:\n",
    "    word_counter.update(tokenize(example))\n",
    "    \n",
    "vocabulary = [pair[0] for pair in word_counter.most_common(top_k)]\n",
    "idx_to_word = dict(enumerate(vocabulary))\n",
    "word_to_idx = dict(zip(idx_to_word.values(), idx_to_word.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract coorcurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_cooccurrences(dataset, word_map, amount_of_context=context_window):\n",
    "    num_words = len(vocabulary)\n",
    "    cooccurrences = np.zeros((num_words, num_words))\n",
    "    nonzero_pairs = set()\n",
    "    for example in dataset:\n",
    "        words = tokenize(example)\n",
    "        for target_index in range(len(words)):\n",
    "            target_word = words[target_index]\n",
    "            if target_word not in word_to_idx:\n",
    "                continue\n",
    "            target_word_index = word_to_idx[target_word]\n",
    "            min_context_index = max(0, target_index - amount_of_context)\n",
    "            max_word = min(len(words), target_index + amount_of_context + 1)\n",
    "            for context_index in list(range(min_context_index, target_index)) + \\\n",
    "            list(range(target_index + 1, max_word)):\n",
    "                context_word = words[context_index]\n",
    "                if context_word not in word_to_idx:\n",
    "                    continue\n",
    "                context_word_index = word_to_idx[context_word]\n",
    "                cooccurrences[target_word_index][context_word_index] += 1.0\n",
    "                nonzero_pairs.add((target_word_index, context_word_index))\n",
    "    return cooccurrences, list(nonzero_pairs)\n",
    "                \n",
    "cooccurrences, nonzero_pairs = extract_cooccurrences(data.item, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(nonzero_pairs, cooccurrences, batch_size):\n",
    "    start = -1 * batch_size\n",
    "    dataset_size = len(nonzero_pairs)\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        word_i = []\n",
    "        word_j = []\n",
    "        counts = []\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [nonzero_pairs[index] for index in batch_indices]\n",
    "        for k in batch:\n",
    "            counts.append(cooccurrences[k])\n",
    "            word_i.append(k[0])\n",
    "            word_j.append(k[1])\n",
    "        yield [counts, word_i, word_j]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be frank, a GloVe model trained on such a small dataset and vocabulary won't be spectacular, so we won't bother with a full-fledged similarity or analogy evaluation. Instead, we'll use the simple scoring function below, which grades the model on how well it captures ten easy/simple similarity comparisons. The function returns a score between 0 and 10. Random embeddings can be expected to get a score of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(model, word_one, word_two):\n",
    "    vec_one = model.get_embeddings(word_to_idx[word_one]).reshape(1, -1)\n",
    "    vec_two = model.get_embeddings(word_to_idx[word_two]).reshape(1, -1)\n",
    "    return float(cosine_similarity(vec_one, vec_two))\n",
    "\n",
    "def score(model):\n",
    "    m = model\n",
    "    score = 0\n",
    "    score += similarity(m, 'cabillaud', 'poisson') > similarity(m, 'cabillaud', 'magret')\n",
    "    score += similarity(m, 'de', 'au') > similarity(m, 'de', 'ou')\n",
    "    score += similarity(m, 'poulet', 'boeuf') >  similarity(m, 'poulet', 'légumes')\n",
    "    score += similarity(m, 'pomme', 'fruits') > similarity(m, 'pomme', 'fromage')\n",
    "    score += similarity(m, 'chocolat', 'vanille') > similarity(m, 'chocolat', 'crème')\n",
    "    score += similarity(m, 'mozzarella', 'fromage') > similarity(m, 'mozzarella', 'miel')\n",
    "    score += similarity(m, 'café', 'thé') > similarity(m, 'café', 'huile')\n",
    "    score += similarity(m, 'entrecôte', 'viande') > similarity(m, 'entrecôte', 'poisson')\n",
    "    score += similarity(m, 'vin', 'champagne') > similarity(m, 'vin', 'soupe')\n",
    "    score += similarity(m, 'confiture', 'nutella') > similarity(m, 'confiture', 'beignets')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = data.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, batch_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.word_embeddings = None\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.co_word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.bias_1 = Variable(torch.FloatTensor(batch_size))\n",
    "        self.bias_2 = Variable(torch.FloatTensor(batch_size))\n",
    "\n",
    "    \n",
    "    def forward(self, counts, words, co_words, xmax, alpha):\n",
    "        \n",
    "        embedded_words = self.word_embeds(words)\n",
    "        embedded_co_words = self.co_word_embeds(co_words)\n",
    "\n",
    "        embed_prod = torch.sum(embedded_words * embedded_co_words, dim=1)\n",
    "        weights = torch.FloatTensor([pow(count/xmax, alpha) if count <= xmax else 1 for count in counts])\n",
    "        \n",
    "        counts = Variable(torch.FloatTensor(counts))\n",
    "        squared_error = torch.pow(embed_prod + self.bias_1 + self.bias_2 - torch.log(counts), 2)\n",
    "        cost = torch.dot(Variable(weights), squared_error)\n",
    "        \n",
    "        if len(weights[weights<0]) > 0:\n",
    "            print('weights negative !')\n",
    "        elif len(squared_error.data[squared_error.data<0]) > 0:\n",
    "            print('squared diff negative !')\n",
    "        #print('cost = ' + str(cost.data[0]))\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    def init_weights(self, i_range):\n",
    "        self.word_embeds.weight.data.uniform_(-i_range, i_range)\n",
    "        self.co_word_embeds.weight.data.uniform_(-i_range, i_range)\n",
    "        \n",
    "        nn.init.uniform(self.bias_1, -i_range/2, i_range/2)\n",
    "        nn.init.uniform(self.bias_2, -i_range/2, i_range/2)\n",
    "    \n",
    "    def add_embeddings(self):\n",
    "\n",
    "        self.word_embeddings = (self.word_embeds.weight + self.co_word_embeds.weight).data.numpy()\n",
    "        return self.word_embeddings\n",
    "    \n",
    "    def get_embeddings(self, index):\n",
    "        if self.word_embeddings is None:\n",
    "            add_embeddings()\n",
    "        return self.word_embeddings[index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, optim, data_iter, xmax, alpha):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        counts, words, co_words = next(data_iter)        \n",
    "        words_var = Variable(torch.LongTensor(words))\n",
    "        co_words_var = Variable(torch.LongTensor(co_words))\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(counts, words_var, co_words_var, xmax, alpha)\n",
    "\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            epoch += 1\n",
    "            if epoch % 25 == 0:\n",
    "                word_embeddings = model.add_embeddings()\n",
    "                print( \"Epoch:\", (epoch), \"Avg Loss:\", np.mean(losses)/(total_batches*epoch), \"Score:\", score(model) )\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Avg Loss: -1.61620822309e+14 Score: 7\n",
      "Epoch: 50 Avg Loss: -6.43360156028e+13 Score: 7\n",
      "Epoch: 75 Avg Loss: -1.09242667079e+12 Score: 6\n",
      "Epoch: 100 Avg Loss: 5.51203022153e+12 Score: 6\n",
      "Epoch: 125 Avg Loss: 1.13444099624e+13 Score: 6\n",
      "Epoch: 150 Avg Loss: 1.11231930651e+13 Score: 6\n",
      "Epoch: 175 Avg Loss: 7.36788674917e+12 Score: 7\n",
      "Epoch: 200 Avg Loss: 4.87527862179e+12 Score: 8\n",
      "Epoch: 225 Avg Loss: 4.33119384456e+12 Score: 8\n",
      "Epoch: 250 Avg Loss: 3.01967699561e+12 Score: 8\n",
      "Epoch: 275 Avg Loss: 2.17280474009e+12 Score: 8\n",
      "Epoch: 300 Avg Loss: 2.16321371024e+12 Score: 8\n",
      "Epoch: 325 Avg Loss: 1.43964351676e+12 Score: 8\n",
      "Epoch: 350 Avg Loss: 1.58855064879e+12 Score: 8\n",
      "Epoch: 375 Avg Loss: 994421697607.0 Score: 8\n",
      "Epoch: 400 Avg Loss: 75987764320.2 Score: 8\n",
      "Epoch: 425 Avg Loss: -168252915667.0 Score: 7\n",
      "Epoch: 450 Avg Loss: 570220541927.0 Score: 7\n",
      "Epoch: 475 Avg Loss: -26932509087.7 Score: 8\n",
      "Epoch: 500 Avg Loss: 243040586690.0 Score: 8\n",
      "Epoch: 525 Avg Loss: 396763581509.0 Score: 8\n",
      "Epoch: 550 Avg Loss: 743048327816.0 Score: 8\n",
      "Epoch: 575 Avg Loss: 734904771276.0 Score: 8\n",
      "Epoch: 600 Avg Loss: 303700599996.0 Score: 8\n",
      "Epoch: 625 Avg Loss: 590840938867.0 Score: 8\n",
      "Epoch: 650 Avg Loss: 1.02059216267e+12 Score: 8\n",
      "Epoch: 675 Avg Loss: 706422384191.0 Score: 8\n",
      "Epoch: 700 Avg Loss: 892297811951.0 Score: 8\n",
      "Epoch: 725 Avg Loss: 623835391353.0 Score: 8\n",
      "Epoch: 750 Avg Loss: 420992738561.0 Score: 8\n",
      "Epoch: 775 Avg Loss: 303271717460.0 Score: 8\n",
      "Epoch: 800 Avg Loss: 104354087088.0 Score: 8\n",
      "Epoch: 825 Avg Loss: 214083815899.0 Score: 8\n",
      "Epoch: 850 Avg Loss: 277294514648.0 Score: 8\n",
      "Epoch: 875 Avg Loss: 118939447193.0 Score: 8\n",
      "Epoch: 900 Avg Loss: 142398599888.0 Score: 8\n",
      "Epoch: 925 Avg Loss: 297982029287.0 Score: 8\n",
      "Epoch: 950 Avg Loss: 47082863498.5 Score: 8\n",
      "Epoch: 975 Avg Loss: 114938024611.0 Score: 8\n",
      "Epoch: 1000 Avg Loss: -157820276202.0 Score: 8\n",
      "Epoch: 1025 Avg Loss: -190653680364.0 Score: 8\n",
      "Epoch: 1050 Avg Loss: -137635487806.0 Score: 8\n",
      "Epoch: 1075 Avg Loss: -57774376610.3 Score: 7\n",
      "Epoch: 1100 Avg Loss: -45144783887.3 Score: 7\n",
      "Epoch: 1125 Avg Loss: -9591062501.95 Score: 7\n",
      "Epoch: 1150 Avg Loss: -45892085910.3 Score: 7\n",
      "Epoch: 1175 Avg Loss: -145065368817.0 Score: 7\n",
      "Epoch: 1200 Avg Loss: -118008549216.0 Score: 7\n",
      "Epoch: 1225 Avg Loss: -198168461361.0 Score: 7\n",
      "Epoch: 1250 Avg Loss: -244694469114.0 Score: 7\n",
      "Epoch: 1275 Avg Loss: -306119067720.0 Score: 7\n",
      "Epoch: 1300 Avg Loss: -319590403296.0 Score: 7\n",
      "Epoch: 1325 Avg Loss: -283443365796.0 Score: 7\n",
      "Epoch: 1350 Avg Loss: -246400955123.0 Score: 7\n",
      "Epoch: 1375 Avg Loss: -202212304698.0 Score: 7\n",
      "Epoch: 1400 Avg Loss: -188860275004.0 Score: 7\n",
      "Epoch: 1425 Avg Loss: -92638911387.1 Score: 7\n",
      "Epoch: 1450 Avg Loss: -135649482245.0 Score: 7\n",
      "Epoch: 1475 Avg Loss: -214762255661.0 Score: 7\n",
      "Epoch: 1500 Avg Loss: -269689418271.0 Score: 7\n",
      "Epoch: 1525 Avg Loss: -279180979305.0 Score: 7\n",
      "Epoch: 1550 Avg Loss: -343488902797.0 Score: 7\n",
      "Epoch: 1575 Avg Loss: -393819816305.0 Score: 7\n",
      "Epoch: 1600 Avg Loss: -293907229850.0 Score: 7\n",
      "Epoch: 1625 Avg Loss: -215995919971.0 Score: 7\n",
      "Epoch: 1650 Avg Loss: -225099178579.0 Score: 7\n",
      "Epoch: 1675 Avg Loss: -227078606650.0 Score: 7\n",
      "Epoch: 1700 Avg Loss: -247740461740.0 Score: 7\n",
      "Epoch: 1725 Avg Loss: -210023623479.0 Score: 7\n",
      "Epoch: 1750 Avg Loss: -210007756989.0 Score: 7\n",
      "Epoch: 1775 Avg Loss: -211835180158.0 Score: 7\n",
      "Epoch: 1800 Avg Loss: -252806081926.0 Score: 7\n",
      "Epoch: 1825 Avg Loss: -238638798128.0 Score: 7\n",
      "Epoch: 1850 Avg Loss: -241094759370.0 Score: 7\n",
      "Epoch: 1875 Avg Loss: -245061466129.0 Score: 7\n",
      "Epoch: 1900 Avg Loss: -236972596843.0 Score: 7\n",
      "Epoch: 1925 Avg Loss: -250503182759.0 Score: 7\n",
      "Epoch: 1950 Avg Loss: -213804087551.0 Score: 7\n",
      "Epoch: 1975 Avg Loss: -213090462028.0 Score: 7\n",
      "Epoch: 2000 Avg Loss: -207795196914.0 Score: 7\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 20\n",
    "vocab_size = len(vocabulary)\n",
    "batch_size = 1024\n",
    "learning_rate = 1.\n",
    "num_epochs = 2000\n",
    "alpha = 0.75\n",
    "xmax = 100\n",
    "\n",
    "glove = Glove(embedding_dim, vocab_size, batch_size)\n",
    "glove.init_weights(0.1)\n",
    "optimizer = torch.optim.Adadelta(glove.parameters(), lr=learning_rate)\n",
    "data_iter = batch_iter(nonzero_pairs, cooccurrences, batch_size)\n",
    "\n",
    "training_loop(batch_size, num_epochs, glove, optimizer, data_iter, xmax, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
