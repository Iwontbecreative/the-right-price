{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe : getting relevant word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import collections\n",
    "import random\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('all_items_improved.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[data.item_price > 0]\n",
    "data = data[data.item_price < 200]\n",
    "data['rest_location'] = data['rest_location'].apply(lambda x: x.split(\"\\n\")[1].strip())\n",
    "data['rest_location'] = data['rest_location'].replace('', '0').replace('Bercy Village', '0').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For speed, we're only using the 250 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_window = 4\n",
    "top_k = 250\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.lower().split()\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for example in data.item:\n",
    "    word_counter.update(tokenize(example))\n",
    "    \n",
    "vocabulary = [pair[0] for pair in word_counter.most_common(top_k)]\n",
    "idx_to_word = dict(enumerate(vocabulary))\n",
    "word_to_idx = dict(zip(idx_to_word.values(), idx_to_word.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract coorcurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_cooccurrences(dataset, word_map, amount_of_context=context_window):\n",
    "    num_words = len(vocabulary)\n",
    "    cooccurrences = np.zeros((num_words, num_words))\n",
    "    nonzero_pairs = set()\n",
    "    for example in dataset:\n",
    "        words = tokenize(example)\n",
    "        for target_index in range(len(words)):\n",
    "            target_word = words[target_index]\n",
    "            if target_word not in word_to_idx:\n",
    "                continue\n",
    "            target_word_index = word_to_idx[target_word]\n",
    "            min_context_index = max(0, target_index - amount_of_context)\n",
    "            max_word = min(len(words), target_index + amount_of_context + 1)\n",
    "            for context_index in list(range(min_context_index, target_index)) + \\\n",
    "            list(range(target_index + 1, max_word)):\n",
    "                context_word = words[context_index]\n",
    "                if context_word not in word_to_idx:\n",
    "                    continue\n",
    "                context_word_index = word_to_idx[context_word]\n",
    "                cooccurrences[target_word_index][context_word_index] += 1.0\n",
    "                nonzero_pairs.add((target_word_index, context_word_index))\n",
    "    return cooccurrences, list(nonzero_pairs)\n",
    "                \n",
    "cooccurrences, nonzero_pairs = extract_cooccurrences(data.item, vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batchify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(nonzero_pairs, cooccurrences, batch_size):\n",
    "    start = -1 * batch_size\n",
    "    dataset_size = len(nonzero_pairs)\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        word_i = []\n",
    "        word_j = []\n",
    "        counts = []\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [nonzero_pairs[index] for index in batch_indices]\n",
    "        for k in batch:\n",
    "            counts.append(cooccurrences[k])\n",
    "            word_i.append(k[0])\n",
    "            word_j.append(k[1])\n",
    "        yield [counts, word_i, word_j]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be frank, a GloVe model trained on such a small dataset and vocabulary won't be spectacular, so we won't bother with a full-fledged similarity or analogy evaluation. Instead, we'll use the simple scoring function below, which grades the model on how well it captures ten easy/simple similarity comparisons. The function returns a score between 0 and 10. Random embeddings can be expected to get a score of 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(model, word_one, word_two):\n",
    "    vec_one = model.get_embeddings(word_to_idx[word_one]).reshape(1, -1)\n",
    "    vec_two = model.get_embeddings(word_to_idx[word_two]).reshape(1, -1)\n",
    "    return float(cosine_similarity(vec_one, vec_two))\n",
    "\n",
    "def score(model):\n",
    "    m = model\n",
    "    score = 0\n",
    "    score += similarity(m, 'cabillaud', 'poisson') > similarity(m, 'cabillaud', 'magret')\n",
    "    score += similarity(m, 'de', 'au') > similarity(m, 'de', 'ou')\n",
    "    score += similarity(m, 'poulet', 'boeuf') >  similarity(m, 'poulet', 'légumes')\n",
    "    score += similarity(m, 'pomme', 'fruits') > similarity(m, 'pomme', 'fromage')\n",
    "    score += similarity(m, 'chocolat', 'vanille') > similarity(m, 'chocolat', 'crème')\n",
    "    score += similarity(m, 'mozzarella', 'fromage') > similarity(m, 'mozzarella', 'miel')\n",
    "    score += similarity(m, 'café', 'thé') > similarity(m, 'café', 'huile')\n",
    "    score += similarity(m, 'entrecôte', 'viande') > similarity(m, 'entrecôte', 'poisson')\n",
    "    score += similarity(m, 'vin', 'champagne') > similarity(m, 'vin', 'soupe')\n",
    "    score += similarity(m, 'confiture', 'nutella') > similarity(m, 'confiture', 'beignets')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set = data.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Glove(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, batch_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.word_embeddings = None\n",
    "        \n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.co_word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.bias_1 = Variable(torch.FloatTensor(batch_size))\n",
    "        self.bias_2 = Variable(torch.FloatTensor(batch_size))\n",
    "\n",
    "    \n",
    "    def forward(self, counts, words, co_words, xmax, alpha):\n",
    "        \n",
    "        embedded_words = self.word_embeds(words)\n",
    "        embedded_co_words = self.co_word_embeds(co_words)\n",
    "\n",
    "        embed_prod = torch.sum(embedded_words * embedded_co_words, dim=1)\n",
    "        weights = torch.FloatTensor([pow(count/xmax, alpha) if count <= xmax else 1 for count in counts])\n",
    "        \n",
    "        counts = Variable(torch.FloatTensor(counts))\n",
    "        squared_error = torch.pow(embed_prod + self.bias_1 + self.bias_2 - torch.log(counts), 2)\n",
    "        cost = torch.dot(Variable(weights), squared_error)\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    def init_weights(self, i_range):\n",
    "        self.word_embeds.weight.data.uniform_(-i_range, i_range)\n",
    "        self.co_word_embeds.weight.data.uniform_(-i_range, i_range)\n",
    "        \n",
    "        nn.init.uniform(self.bias_1, -i_range/2, i_range/2)\n",
    "        nn.init.uniform(self.bias_2, -i_range/2, i_range/2)\n",
    "    \n",
    "    def add_embeddings(self):\n",
    "\n",
    "        self.word_embeddings = (self.word_embeds.weight + self.co_word_embeds.weight).data.numpy()\n",
    "        return self.word_embeddings\n",
    "    \n",
    "    def get_embeddings(self, index):\n",
    "        if self.word_embeddings is None:\n",
    "            add_embeddings()\n",
    "        return self.word_embeddings[index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, optim, data_iter, xmax, alpha):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    losses = []\n",
    "    total_batches = int(len(training_set) / batch_size)\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        counts, words, co_words = next(data_iter)        \n",
    "        words_var = Variable(torch.LongTensor(words))\n",
    "        co_words_var = Variable(torch.LongTensor(co_words))\n",
    "        \n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = model(counts, words_var, co_words_var, xmax, alpha)\n",
    "\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            epoch += 1\n",
    "            if epoch % 25 == 0:\n",
    "                word_embeddings = model.add_embeddings()\n",
    "                print( \"Epoch:\", (epoch), \"Avg Loss:\", np.mean(losses)/(total_batches*epoch), \"Score:\", score(model) )\n",
    "        \n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 Avg Loss: 0.208974671265 Score: 8\n",
      "Epoch: 50 Avg Loss: 0.0637885843842 Score: 6\n",
      "Epoch: 75 Avg Loss: 0.0312778128672 Score: 6\n",
      "Epoch: 100 Avg Loss: 0.0189935303256 Score: 7\n",
      "Epoch: 125 Avg Loss: 0.0130070159012 Score: 7\n",
      "Epoch: 150 Avg Loss: 0.00961004500239 Score: 7\n",
      "Epoch: 175 Avg Loss: 0.00747865691775 Score: 7\n",
      "Epoch: 200 Avg Loss: 0.00604313646877 Score: 7\n",
      "Epoch: 225 Avg Loss: 0.0050235435498 Score: 7\n",
      "Epoch: 250 Avg Loss: 0.00426963866347 Score: 7\n",
      "Epoch: 275 Avg Loss: 0.0036936758504 Score: 7\n",
      "Epoch: 300 Avg Loss: 0.00324194624476 Score: 7\n",
      "Epoch: 325 Avg Loss: 0.00287975880561 Score: 7\n",
      "Epoch: 350 Avg Loss: 0.00258409297121 Score: 7\n",
      "Epoch: 375 Avg Loss: 0.00233892013192 Score: 7\n",
      "Epoch: 400 Avg Loss: 0.00213280880236 Score: 7\n",
      "Epoch: 425 Avg Loss: 0.00195743367815 Score: 7\n",
      "Epoch: 450 Avg Loss: 0.00180674160806 Score: 7\n",
      "Epoch: 475 Avg Loss: 0.00167605373138 Score: 7\n",
      "Epoch: 500 Avg Loss: 0.00156176104318 Score: 7\n",
      "Epoch: 525 Avg Loss: 0.00146109880642 Score: 7\n",
      "Epoch: 550 Avg Loss: 0.00137188742125 Score: 7\n",
      "Epoch: 575 Avg Loss: 0.00129226339256 Score: 7\n",
      "Epoch: 600 Avg Loss: 0.00122089373798 Score: 7\n",
      "Epoch: 625 Avg Loss: 0.00115655930145 Score: 6\n",
      "Epoch: 650 Avg Loss: 0.00109832021683 Score: 7\n",
      "Epoch: 675 Avg Loss: 0.00104536979091 Score: 6\n",
      "Epoch: 700 Avg Loss: 0.00099705242181 Score: 6\n",
      "Epoch: 725 Avg Loss: 0.000952794353058 Score: 6\n",
      "Epoch: 750 Avg Loss: 0.000912145004986 Score: 6\n",
      "Epoch: 775 Avg Loss: 0.000874667014666 Score: 6\n",
      "Epoch: 800 Avg Loss: 0.00084002124512 Score: 6\n",
      "Epoch: 825 Avg Loss: 0.000807890747946 Score: 6\n",
      "Epoch: 850 Avg Loss: 0.000778059818483 Score: 6\n",
      "Epoch: 875 Avg Loss: 0.000750270932647 Score: 6\n",
      "Epoch: 900 Avg Loss: 0.000724329075463 Score: 6\n",
      "Epoch: 925 Avg Loss: 0.000700055828971 Score: 6\n",
      "Epoch: 950 Avg Loss: 0.000677307225247 Score: 6\n",
      "Epoch: 975 Avg Loss: 0.000655943150518 Score: 6\n",
      "Epoch: 1000 Avg Loss: 0.000635847135147 Score: 6\n",
      "Epoch: 1025 Avg Loss: 0.000616912108793 Score: 6\n",
      "Epoch: 1050 Avg Loss: 0.000599049453209 Score: 6\n",
      "Epoch: 1075 Avg Loss: 0.000582158382089 Score: 6\n",
      "Epoch: 1100 Avg Loss: 0.000566168751694 Score: 6\n",
      "Epoch: 1125 Avg Loss: 0.000551019034183 Score: 6\n",
      "Epoch: 1150 Avg Loss: 0.000536641047378 Score: 6\n",
      "Epoch: 1175 Avg Loss: 0.000522976126359 Score: 6\n",
      "Epoch: 1200 Avg Loss: 0.000509983543996 Score: 6\n",
      "Epoch: 1225 Avg Loss: 0.000497605853608 Score: 6\n",
      "Epoch: 1250 Avg Loss: 0.000485798404913 Score: 6\n",
      "Epoch: 1275 Avg Loss: 0.000474545138397 Score: 6\n",
      "Epoch: 1300 Avg Loss: 0.000463781030959 Score: 6\n",
      "Epoch: 1325 Avg Loss: 0.00045349171123 Score: 6\n",
      "Epoch: 1350 Avg Loss: 0.000443633176049 Score: 6\n",
      "Epoch: 1375 Avg Loss: 0.000434194151525 Score: 6\n",
      "Epoch: 1400 Avg Loss: 0.00042514253636 Score: 6\n",
      "Epoch: 1425 Avg Loss: 0.000416450480113 Score: 6\n",
      "Epoch: 1450 Avg Loss: 0.000408106867755 Score: 7\n",
      "Epoch: 1475 Avg Loss: 0.000400088790855 Score: 7\n",
      "Epoch: 1500 Avg Loss: 0.000392371577421 Score: 7\n",
      "Epoch: 1525 Avg Loss: 0.000384949522271 Score: 7\n",
      "Epoch: 1550 Avg Loss: 0.000377793010724 Score: 7\n",
      "Epoch: 1575 Avg Loss: 0.000370902231237 Score: 7\n",
      "Epoch: 1600 Avg Loss: 0.000364257206533 Score: 7\n",
      "Epoch: 1625 Avg Loss: 0.000357846158402 Score: 7\n",
      "Epoch: 1650 Avg Loss: 0.000351653251392 Score: 7\n",
      "Epoch: 1675 Avg Loss: 0.000345668834787 Score: 7\n",
      "Epoch: 1700 Avg Loss: 0.000339891575948 Score: 7\n",
      "Epoch: 1725 Avg Loss: 0.000334294687583 Score: 7\n",
      "Epoch: 1750 Avg Loss: 0.000328882858921 Score: 7\n",
      "Epoch: 1775 Avg Loss: 0.000323643794329 Score: 7\n",
      "Epoch: 1800 Avg Loss: 0.000318568480158 Score: 7\n",
      "Epoch: 1825 Avg Loss: 0.000313649521689 Score: 6\n",
      "Epoch: 1850 Avg Loss: 0.00030888391378 Score: 6\n",
      "Epoch: 1875 Avg Loss: 0.000304259302403 Score: 6\n",
      "Epoch: 1900 Avg Loss: 0.000299773665141 Score: 5\n",
      "Epoch: 1925 Avg Loss: 0.000295415322726 Score: 5\n",
      "Epoch: 1950 Avg Loss: 0.000291181447927 Score: 6\n",
      "Epoch: 1975 Avg Loss: 0.000287068206857 Score: 6\n",
      "Epoch: 2000 Avg Loss: 0.000283071391984 Score: 5\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 20\n",
    "vocab_size = len(vocabulary)\n",
    "batch_size = 1024\n",
    "learning_rate = 1.\n",
    "num_epochs = 2000\n",
    "alpha = 0.75\n",
    "xmax = 100\n",
    "\n",
    "glove = Glove(embedding_dim, vocab_size, batch_size)\n",
    "glove.init_weights(0.1)\n",
    "optimizer = torch.optim.Adadelta(glove.parameters(), lr=learning_rate)\n",
    "data_iter = batch_iter(nonzero_pairs, cooccurrences, batch_size)\n",
    "\n",
    "training_loop(batch_size, num_epochs, glove, optimizer, data_iter, xmax, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
